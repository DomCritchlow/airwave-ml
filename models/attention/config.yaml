# Attention-based Seq2Seq Audio-to-Text Model
# Architecture: CNN + Transformer Encoder-Decoder with cross-attention
#
# This model uses:
# - Teacher forcing during training
# - Autoregressive decoding (beam search or greedy)
# - Language modeling capability (can "correct" errors but may hallucinate)
#
# Best for:
# - Natural language transcription
# - Error correction is desired
# - Context-dependent decoding

model:
  d_model: 192              # Embedding dimension
  nhead: 6                  # Attention heads
  num_encoder_layers: 3     # Encoder depth
  num_decoder_layers: 3     # Decoder depth
  dim_feedforward: 768      # FFN dimension
  dropout: 0.15             # Regularization
  # Total: ~3.4M parameters

audio:
  sample_rate: 16000
  n_mels: 80
  n_fft: 400
  hop_length: 160
  feature_type: "mel"

training:
  batch_size: 16
  epochs: 100
  learning_rate: 0.0001
  warmup_epochs: 5
  num_workers: 0

data:
  max_audio_len: 2000
  max_text_len: 200

early_stopping:
  enabled: true
  patience: 7
  min_delta: 0.001

paths:
  data_dir: ../../data/synthetic/morse_v1
  checkpoint_dir: checkpoints
  log_dir: logs

vocab: " 0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ"

