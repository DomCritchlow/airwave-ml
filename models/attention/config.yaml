# Attention-based Seq2Seq Audio-to-Text Model
# Architecture: CNN + Transformer Encoder-Decoder with cross-attention
#
# This model uses:
# - Teacher forcing during training
# - Autoregressive decoding (beam search or greedy)
# - Language modeling capability (can "correct" errors but may hallucinate)
#
# Best for:
# - Natural language transcription
# - Error correction is desired
# - Context-dependent decoding

model:
  d_model: 192              # Embedding dimension
  nhead: 6                  # Attention heads
  num_encoder_layers: 3     # Encoder depth
  num_decoder_layers: 3     # Decoder depth
  dim_feedforward: 768      # FFN dimension
  dropout: 0.15             # Regularization
  # Total: ~3.4M parameters

audio:
  sample_rate: 16000
  n_mels: 80
  n_fft: 400
  hop_length: 160
  feature_type: "mel"

training:
  batch_size: 16
  epochs: 100
  learning_rate: 0.0001
  weight_decay: 0.01
  warmup_epochs: 5
  scheduler: "cosine"
  num_workers: 4

data:
  max_audio_len: 2000
  max_text_len: 200

# Audio augmentation
augmentation:
  enabled: true
  prob_mp3_compression: 0.3
  prob_bandpass: 0.3
  prob_time_stretch: 0.2
  prob_pitch_shift: 0.2
  prob_noise: 0.4
  prob_volume: 0.3
  prob_clipping: 0.1
  prob_lowpass: 0.2
  prob_reverb: 0.1

early_stopping:
  enabled: true
  patience: 10
  min_delta: 0.001

checkpoint:
  save_every: 10
  save_best: true

logging:
  use_tensorboard: true
  log_every: 50

paths:
  data_dir: ../../data/synthetic/morse_v2
  checkpoint_dir: checkpoints
  log_dir: runs

# Vocabulary
vocab: " -0123456789?ABCDEFGHIJKLMNOPQRSTUVWXYZ"
