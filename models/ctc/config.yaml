# CTC-based Audio-to-Text Model
# Architecture: CNN + Transformer Encoder (no decoder) + CTC loss
#
# Robust model for real-world morse decoding
# Trained with heavy augmentation for generalization

model:
  hidden_dim: 256           # Larger for complex patterns
  num_layers: 4             # Deeper for better representations
  dropout: 0.1              # Standard regularization
  encoder_type: "transformer"
  nhead: 8                  # More attention heads
  # Total: ~2M parameters

audio:
  sample_rate: 16000
  n_mels: 80
  n_fft: 400
  hop_length: 160

training:
  batch_size: 32            # Larger batch for stability
  epochs: 150               # More epochs for larger dataset
  learning_rate: 0.0005     # Slightly higher LR
  num_workers: 4            # Parallel data loading

data:
  max_audio_len: 2000
  max_text_len: 200

# Audio augmentation for robust training
# Forces model to learn Morse patterns, not audio characteristics
augmentation:
  enabled: true
  prob_mp3_compression: 0.3   # Simulate lossy compression
  prob_bandpass: 0.3          # Simulate radio bandwidth
  prob_time_stretch: 0.2      # Speed variations
  prob_pitch_shift: 0.2       # Frequency variations
  prob_noise: 0.4             # Various noise types
  prob_volume: 0.3            # Volume variations
  prob_clipping: 0.1          # Overdriven audio
  prob_lowpass: 0.2           # Poor audio quality
  prob_reverb: 0.1            # Room acoustics

early_stopping:
  patience: 15                # More patience with augmentation
  min_delta: 0.002            # Require 0.2% CER improvement

paths:
  data_dir: ../../data/synthetic/morse_v2
  checkpoint_dir: checkpoints
  log_dir: logs

# Vocabulary (BLANK token added automatically at index 0)
vocab: " -0123456789?ABCDEFGHIJKLMNOPQRSTUVWXYZ"

